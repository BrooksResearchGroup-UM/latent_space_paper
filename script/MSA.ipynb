{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exclude the sequences outside of the range of 300â€“600 aa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio import SeqIO\n",
    "\n",
    "def filter_fasta_by_length(input_fasta, output_fasta, min_length=300, max_length=600):\n",
    "    # Parse the input FASTA file and filter sequences based on length\n",
    "    filtered_records = [record for record in SeqIO.parse(input_fasta, \"fasta\") \n",
    "                        if min_length <= len(record.seq) <= max_length]\n",
    "\n",
    "    # Write the filtered sequences to the output FASTA file\n",
    "    SeqIO.write(filtered_records, output_fasta, \"fasta\")\n",
    "    \n",
    "# Training dataset\n",
    "input_fasta_file = \"../data/PF01494_20201216.fasta\"\n",
    "output_fasta_file = \"../data/PF01494_20201216_300_600aa.fasta\"\n",
    "filter_fasta_by_length(input_fasta_file, output_fasta_file)\n",
    "\n",
    "# Testing dataset\n",
    "input_fasta_file = \"../data/PF01494_testing_dataset_20230916_markdup.fasta\"\n",
    "output_fasta_file = \"../data/PF01494_testing_dataset_20230916_markdup_300_600aa.fasta\"\n",
    "filter_fasta_by_length(input_fasta_file, output_fasta_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Process the train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train binary shape:  (33937, 353, 21)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import sys\n",
    "import numpy as np\n",
    "from sys import exit\n",
    "from Bio import SeqIO\n",
    "from Bio.Seq import Seq\n",
    "from Bio.SeqRecord import SeqRecord\n",
    "\n",
    "# Function to split sequences into training and testing datasets\n",
    "def split_fasta(input_fasta, output_fasta1, output_fasta2, split_index=39948):\n",
    "    # Load all sequences from the input FASTA into a list\n",
    "    all_sequences = list(SeqIO.parse(input_fasta, \"fasta\"))\n",
    "    \n",
    "    # Split the list into two parts\n",
    "    train_dataset = all_sequences[:split_index]\n",
    "    test_dataset = all_sequences[split_index:]\n",
    "\n",
    "    # Write sequences to the respective output files\n",
    "    SeqIO.write(train_dataset, output_fasta1, \"fasta\")\n",
    "    SeqIO.write(test_dataset, output_fasta2, \"fasta\")\n",
    "    \n",
    "    # print(f\"First part written to {output_fasta1}\")\n",
    "    # print(f\"Second part written to {output_fasta2}\")\n",
    "\n",
    "    return train_dataset, test_dataset\n",
    "\n",
    "# 33948 sequences in the train dataset\n",
    "# 1550 sequences in the test dataset\n",
    "# Specify your file names\n",
    "input_fasta_file = \"../data/PF01494_20201216_300_600aa_train_test_MSA.fasta\"\n",
    "output_fasta_file1 = \"../data/PF01494_20201216_300_600aa_train_MSA.fasta\"\n",
    "output_fasta_file2 = \"../data/PF01494_20201216_300_600aa_test_MSA.fasta\"\n",
    "\n",
    "train_dataset, test_dataset = split_fasta(input_fasta_file, output_fasta_file1, output_fasta_file2)\n",
    "\n",
    "MSA_file = \"../data/PF01494_20201216_300_600aa_train_MSA.fasta\"\n",
    "query_seq_id = \"B8M9J8\"\n",
    "\n",
    "# Define a list of amino acid characters\n",
    "AA = ['R', 'H', 'K',\n",
    "      'D', 'E',\n",
    "      'S', 'T', 'N', 'Q',\n",
    "      'C', 'G', 'P',\n",
    "      'A', 'V', 'I', 'L', 'M', 'F', 'Y', 'W']\n",
    "\n",
    "aa_to_index = {}\n",
    "aa_to_index['-'] = 0\n",
    "aa_to_index['.'] = 0\n",
    "index_to_aa = {}\n",
    "index_to_aa[0] = '-'\n",
    "for idx, aa in enumerate(AA, start=1):\n",
    "    aa_to_index[aa] = idx\n",
    "    index_to_aa[idx] = aa\n",
    "\n",
    "# Save aa_to_index\n",
    "with open(\"../data/encoding/aa_to_index.pkl\", 'wb') as file_handle:\n",
    "    pickle.dump(aa_to_index, file_handle)\n",
    "\n",
    "# Save index_to_aa\n",
    "with open(\"../data/encoding/index_to_aa.pkl\", 'wb') as file_handle:\n",
    "    pickle.dump(index_to_aa, file_handle)\n",
    "\n",
    "# Read all the sequences into a dictionary\n",
    "def MSA_to_dict(MSA_file):\n",
    "    seq_dict = {record.id: str(record.seq).upper() for record in SeqIO.parse(MSA_file, \"fasta\")}\n",
    "    return seq_dict\n",
    "\n",
    "# Remove a column if the query has a gap at that position\n",
    "def remove_query_gap(seq_dict, query_seq_id):\n",
    "\n",
    "    query_seq = seq_dict[query_seq_id] ## potentially with gaps\n",
    "    idx = [ s == \"-\" or s == \".\" for s in query_seq]\n",
    "    for k in seq_dict.keys():\n",
    "        seq_dict[k] = [seq_dict[k][i] for i in range(len(seq_dict[k])) if idx[i] == False]\n",
    "    query_seq = seq_dict[query_seq_id] ## without gaps\n",
    "\n",
    "    return seq_dict, query_seq\n",
    "\n",
    "# Remove gappy sequences\n",
    "def remove_gappy_sequneces(seq_dict, query_seq_id):\n",
    "\n",
    "    len_query_seq = len(query_seq)\n",
    "    deleted_id_list = []\n",
    "    deletion_reason = []\n",
    "    for k in list(seq_dict.keys()):\n",
    "        if seq_dict[k].count('X') > 0 or seq_dict[k].count('Z') > 0 or seq_dict[k].count('O') > 0:\n",
    "            seq_dict.pop(k)\n",
    "            deleted_id_list.append(k)\n",
    "            deletion_reason.append('XZO')\n",
    "\n",
    "        elif seq_dict[k].count(\"-\") + seq_dict[k].count(\".\") > 0.2*len_query_seq:\n",
    "            seq_dict.pop(k)\n",
    "            deleted_id_list.append(k)\n",
    "            deletion_reason.append('gappy')\n",
    "\n",
    "    return seq_dict, deleted_id_list, deletion_reason\n",
    "\n",
    "seq_dict = MSA_to_dict(MSA_file)\n",
    "MSA_query = seq_dict[query_seq_id]\n",
    "\n",
    "# Save MSA query seq\n",
    "with open(\"../data/encoding/MSA_query.pkl\", 'wb') as file_handle:\n",
    "    pickle.dump(MSA_query, file_handle)\n",
    "\n",
    "seq_dict, query_seq = remove_query_gap(seq_dict, query_seq_id)\n",
    "\n",
    "# Save nogap query seq\n",
    "with open(\"../data/encoding/nogap_query.pkl\", 'wb') as file_handle:\n",
    "    pickle.dump(query_seq, file_handle)\n",
    "\n",
    "seq_dict, deleted_id_list, deletion_reason = remove_gappy_sequneces(seq_dict, query_seq_id)\n",
    "\n",
    "# Save deleted_id_list\n",
    "with open(\"../data/encoding/deleted_id_list.pkl\", 'wb') as file_handle:\n",
    "    pickle.dump(deleted_id_list, file_handle)\n",
    "\n",
    "id_list = []\n",
    "seq_msa = []\n",
    "for k in seq_dict.keys():\n",
    "    id_list.append(k)\n",
    "    seq_msa.append([aa_to_index[s] for s in seq_dict[k]])\n",
    "seq_msa = np.array(seq_msa)\n",
    "\n",
    "# Remove positions where too many sequences have gaps\n",
    "pos_idx = []\n",
    "for i in range(seq_msa.shape[1]):\n",
    "    if np.sum(seq_msa[:,i] == 0) <= seq_msa.shape[0]*0.2:\n",
    "        pos_idx.append(i)\n",
    "\n",
    "with open(\"../data/encoding/seq_pos_idx.pkl\", 'wb') as file_handle:\n",
    "    pickle.dump(pos_idx, file_handle)\n",
    "\n",
    "seq_msa = seq_msa[:, np.array(pos_idx)]\n",
    "\n",
    "# Reweight sequences\n",
    "seq_weight = np.zeros(seq_msa.shape)\n",
    "for j in range(seq_msa.shape[1]):\n",
    "    aa_type, aa_counts = np.unique(seq_msa[:,j], return_counts = True)\n",
    "    num_type = len(aa_type)\n",
    "    aa_dict = {}\n",
    "    for a in aa_type:\n",
    "        aa_dict[a] = aa_counts[list(aa_type).index(a)]\n",
    "    for i in range(seq_msa.shape[0]):\n",
    "        seq_weight[i,j] = (1.0/num_type) * (1.0/aa_dict[seq_msa[i,j]])\n",
    "tot_weight = np.sum(seq_weight)\n",
    "seq_weight = seq_weight.sum(1) / tot_weight\n",
    "\n",
    "# Save seq_weight\n",
    "with open(\"../data/encoding/seq_weight.pkl\", 'wb') as file_handle:\n",
    "    pickle.dump(seq_weight, file_handle)\n",
    "\n",
    "# Save seq_msa (one-hot encoded version)\n",
    "with open(\"../data/encoding/seq_msa.pkl\", 'wb') as file_handle:\n",
    "    pickle.dump(seq_msa, file_handle)\n",
    "\n",
    "# Decode idx back to aa\n",
    "seq_msa_aa = []\n",
    "for k in range(seq_msa.shape[0]):\n",
    "    decoded_seq = \"\".join(index_to_aa[idx] for idx in seq_msa[k])\n",
    "    seq_msa_aa.append(decoded_seq)\n",
    "\n",
    "seq_dict_truncated = {id: seq for id, seq in zip(id_list, seq_msa_aa)}\n",
    "\n",
    "# Save seq_dict_truncated\n",
    "with open(\"../data/encoding/seq_dict_truncated.pkl\", 'wb') as file_handle:\n",
    "     pickle.dump(seq_dict_truncated, file_handle)\n",
    "\n",
    "# Save the position number of the residues that are kept\n",
    "template_pos_idx = []\n",
    "for i, res in enumerate(MSA_query):\n",
    "    # If the sequence is empty, break the loop\n",
    "    if not seq_dict_truncated[query_seq_id]:\n",
    "        break\n",
    "    if res == seq_dict_truncated[query_seq_id][0]:\n",
    "        template_pos_idx.append(i+1)\n",
    "        seq_dict_truncated[query_seq_id] = seq_dict_truncated[query_seq_id][1:]\n",
    "\n",
    "# Save template_pos_idx\n",
    "with open(\"../data/encoding/template_pos_idx.pkl\", 'wb') as file_handle:\n",
    "     pickle.dump(template_pos_idx, file_handle)\n",
    "\n",
    "# Save keys_list\n",
    "with open(\"../data/encoding/keys_list.pkl\", 'wb') as file_handle:\n",
    "    pickle.dump(id_list, file_handle)\n",
    "\n",
    "# Save seq_dict_truncated\n",
    "with open(\"../data/encoding/seq_dict_truncated.pkl\", 'wb') as file_handle:\n",
    "    pickle.dump(seq_dict_truncated, file_handle)\n",
    "\n",
    "# Change aa numbering into binary\n",
    "K = 21 ## num of classes of aa\n",
    "D = np.identity(K)\n",
    "num_seq = seq_msa.shape[0]\n",
    "len_seq_msa = seq_msa.shape[1]\n",
    "seq_msa_binary = np.zeros((num_seq, len_seq_msa, K))\n",
    "for i in range(num_seq):\n",
    "    seq_msa_binary[i,:,:] = D[seq_msa[i]]\n",
    "\n",
    "# Save seq_msa_binary\n",
    "with open(\"../data/encoding/seq_msa_binary.pkl\", 'wb') as file_handle:\n",
    "    pickle.dump(seq_msa_binary, file_handle)\n",
    "\n",
    "# Print train_seq_msa_binary shape\n",
    "print('Train binary shape: ', seq_msa_binary.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Process the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test binary shape:  (1389, 353, 21)\n"
     ]
    }
   ],
   "source": [
    "test_MSA_file = \"../data/PF01494_20201216_300_600aa_test_MSA.fasta\"\n",
    "test_seq_dict = MSA_to_dict(test_MSA_file)\n",
    "query_seq_id = 'B8M9J8'\n",
    "\n",
    "with open('../data/encoding/aa_to_index.pkl', 'rb') as file:\n",
    "    aa_to_index = pickle.load(file)\n",
    "\n",
    "with open('../data/encoding/index_to_aa.pkl', 'rb') as file:\n",
    "    index_to_aa = pickle.load(file)\n",
    "\n",
    "with open('../data/encoding/MSA_query.pkl', 'rb') as file:\n",
    "    MSA_query = pickle.load(file)\n",
    "\n",
    "with open('../data/encoding/nogap_query.pkl', 'rb') as file:\n",
    "    query_seq = pickle.load(file)\n",
    "\n",
    "with open('../data/encoding/seq_pos_idx.pkl', 'rb') as file:\n",
    "    pos_idx = pickle.load(file)\n",
    "\n",
    "# Remove a column if the query has a gap at that position\n",
    "def remove_test_query_gap(seq_dict, MSA_query):\n",
    "\n",
    "    idx = [ s == \"-\" or s == \".\" for s in MSA_query]\n",
    "    for k in seq_dict.keys():\n",
    "        seq_dict[k] = [seq_dict[k][i] for i in range(len(seq_dict[k])) if idx[i] == False]\n",
    "\n",
    "    return seq_dict\n",
    "\n",
    "# Remove gappy sequences\n",
    "def remove_test_gappy_sequneces(seq_dict, query_seq):\n",
    "\n",
    "    len_query_seq = len(query_seq)\n",
    "    deleted_id_list = []\n",
    "    deletion_reason = []\n",
    "    for k in list(seq_dict.keys()):\n",
    "        if seq_dict[k].count('X') > 0 or seq_dict[k].count('Z') > 0 or seq_dict[k].count('O') > 0:\n",
    "            seq_dict.pop(k)\n",
    "            deleted_id_list.append(k)\n",
    "            deletion_reason.append('XZO')\n",
    "\n",
    "        elif seq_dict[k].count(\"-\") + seq_dict[k].count(\".\") > 0.2*len_query_seq:\n",
    "            seq_dict.pop(k)\n",
    "            deleted_id_list.append(k)\n",
    "            deletion_reason.append('gappy')\n",
    "\n",
    "    return seq_dict, deleted_id_list, deletion_reason\n",
    "\n",
    "test_seq_dict = remove_test_query_gap(test_seq_dict, MSA_query)\n",
    "test_seq_dict, test_deleted_id_list, test_deletion_reason = remove_gappy_sequneces(test_seq_dict, query_seq_id)\n",
    "\n",
    "# Save deleted_id_list\n",
    "with open(\"../data/encoding/test_deleted_id_list.pkl\", 'wb') as file_handle:\n",
    "    pickle.dump(test_deleted_id_list, file_handle)\n",
    "\n",
    "test_id_list = []\n",
    "test_seq_msa = []\n",
    "for k in test_seq_dict.keys():\n",
    "    test_id_list.append(k)\n",
    "    test_seq_msa.append([aa_to_index[s] for s in test_seq_dict[k]])\n",
    "test_seq_msa = np.array(test_seq_msa)\n",
    "\n",
    "# Remove positions where too many sequences have gaps\n",
    "test_seq_msa = test_seq_msa[:, np.array(pos_idx)]\n",
    "\n",
    "# Assuming seq_msa_test represents your test sequences\n",
    "num_test_samples = test_seq_msa.shape[0]\n",
    "# Create an array of ones with a length equal to the number of test sequences\n",
    "test_seq_weight = np.ones(num_test_samples)\n",
    "\n",
    "# Save seq_weight\n",
    "with open(\"../data/encoding/test_seq_weight.pkl\", 'wb') as file_handle:\n",
    "    pickle.dump(test_seq_weight, file_handle)\n",
    "\n",
    "# Save seq_msa (one-hot encoded version)\n",
    "with open(\"../data/encoding/test_seq_msa.pkl\", 'wb') as file_handle:\n",
    "    pickle.dump(test_seq_msa, file_handle)\n",
    "\n",
    "# Decode idx back to aa\n",
    "test_seq_msa_aa = []\n",
    "for k in range(test_seq_msa.shape[0]):\n",
    "    decoded_seq = \"\".join(index_to_aa[idx] for idx in test_seq_msa[k])\n",
    "    test_seq_msa_aa.append(decoded_seq)\n",
    "\n",
    "test_seq_dict_truncated = {id: seq for id, seq in zip(id_list, test_seq_msa_aa)}\n",
    "\n",
    "# Save seq_dict_truncated\n",
    "with open(\"../data/encoding/test_seq_dict_truncated.pkl\", 'wb') as file_handle:\n",
    "     pickle.dump(test_seq_dict_truncated, file_handle)\n",
    "\n",
    "# Save keys_list\n",
    "with open(\"../data/encoding/test_keys_list.pkl\", 'wb') as file_handle:\n",
    "    pickle.dump(test_id_list, file_handle)\n",
    "\n",
    "# Change aa numbering into binary\n",
    "K = 21 ## num of classes of aa\n",
    "D = np.identity(K)\n",
    "num_seq = test_seq_msa.shape[0]\n",
    "len_seq_msa = test_seq_msa.shape[1]\n",
    "test_seq_msa_binary = np.zeros((num_seq, len_seq_msa, K))\n",
    "for i in range(num_seq):\n",
    "    test_seq_msa_binary[i,:,:] = D[test_seq_msa[i]]\n",
    "\n",
    "# Save seq_msa_binary\n",
    "with open(\"../data/encoding/test_seq_msa_binary.pkl\", 'wb') as file_handle:\n",
    "    pickle.dump(test_seq_msa_binary, file_handle)\n",
    "\n",
    "# Print test_seq_msa_binary shape\n",
    "print('Test binary shape: ', test_seq_msa_binary.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
